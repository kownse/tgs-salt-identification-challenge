{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook #, tnrange\n",
    "#from itertools import chain\n",
    "from skimage.io import imread, imshow #, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\n",
    "\n",
    "import time\n",
    "from kaggle_util import *\n",
    "from models import *\n",
    "\n",
    "from albumentations import PadIfNeeded\n",
    "\n",
    "t_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "\n",
    "def build_model_deeper(input_layer, start_neurons, DropoutRatio = 0.5):\n",
    "    # 101 -> 50\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(input_layer)\n",
    "    conv1 = residual_block(conv1,start_neurons * 1)\n",
    "    conv1 = residual_block(conv1,start_neurons * 1)\n",
    "    conv1 = residual_block(conv1,start_neurons * 1, True)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(DropoutRatio/2)(pool1)\n",
    "\n",
    "    # 50 -> 25\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n",
    "    conv2 = residual_block(conv2,start_neurons * 2)\n",
    "    conv2 = residual_block(conv2,start_neurons * 2)\n",
    "    conv2 = residual_block(conv2,start_neurons * 2, True)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(DropoutRatio)(pool2)\n",
    "\n",
    "    # 25 -> 12\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n",
    "    conv3 = residual_block(conv3,start_neurons * 4)\n",
    "    conv3 = residual_block(conv3,start_neurons * 4)\n",
    "    conv3 = residual_block(conv3,start_neurons * 4, True)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(DropoutRatio)(pool3)\n",
    "\n",
    "    # 12 -> 6\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n",
    "    conv4 = residual_block(conv4,start_neurons * 8)\n",
    "    conv4 = residual_block(conv4,start_neurons * 8)\n",
    "    conv4 = residual_block(conv4,start_neurons * 8, True)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(DropoutRatio)(pool4)\n",
    "\n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n",
    "    convm = residual_block(convm,start_neurons * 16)\n",
    "    convm = residual_block(convm,start_neurons * 16)\n",
    "    convm = residual_block(convm,start_neurons * 16, True)\n",
    "    \n",
    "    # 6 -> 12\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(DropoutRatio)(uconv4)\n",
    "    \n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 8)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 8)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n",
    "    \n",
    "    # 12 -> 25\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "#     deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])    \n",
    "    uconv3 = Dropout(DropoutRatio)(uconv3)\n",
    "    \n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 4)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 4)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n",
    "\n",
    "    # 25 -> 50\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "        \n",
    "    uconv2 = Dropout(DropoutRatio)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n",
    "    \n",
    "    # 50 -> 101\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "#     deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    \n",
    "    uconv1 = Dropout(DropoutRatio)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 1, True)\n",
    "    \n",
    "    #uconv1 = Dropout(DropoutRatio/2)(uconv1)\n",
    "    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\n",
    "    output_layer =  Activation('sigmoid')(output_layer_noActi)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1ab8b2a2be40b598405cd35561c030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kownse/.local/lib/python3.6/site-packages/keras_preprocessing/image.py:487: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4696a47ebc4b0889b93ea710c0ec53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loading of training/testing ids and depths\n",
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "\n",
    "len(train_df)\n",
    "\n",
    "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBSET = len(train_df)\n",
    "train_df = train_df.head(SUBSET)\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "train_df.index.values,\n",
    "np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "train_df.coverage.values,\n",
    "train_df.z.values,\n",
    "test_size=0.2, stratify=train_df.coverage_class, random_state= 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 128, 128, 1)\n",
      "(800, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "#Data augmentation\n",
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n",
    "print(x_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/resunet_scale128_32_0.5.model\n",
      "../model/resunet_scale128_32_0.5.csv\n"
     ]
    }
   ],
   "source": [
    "start_feature = 32\n",
    "dropout = 0.5\n",
    "basic_name = '../model/resunet_scale128_{}_{}'.format(start_feature, dropout)\n",
    "save_model_name = basic_name + '.model'\n",
    "submission_file = basic_name + '.csv'\n",
    "\n",
    "print(save_model_name)\n",
    "print(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model\n",
    "input_layer = Input((img_size_target, img_size_target, 1))\n",
    "output_layer = build_model_deeper(input_layer, start_feature,dropout)\n",
    "\n",
    "model1 = Model(input_layer, output_layer)\n",
    "\n",
    "c = optimizers.adam(lr = 0.01)\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=c, metrics=[my_iou_metric])\n",
    "\n",
    "board = keras.callbacks.TensorBoard(log_dir='log/resunet_scale128_{}_{}'.format(start_feature, dropout),\n",
    "                       histogram_freq=0, write_graph=True, write_images=False)\n",
    "early_stopping = EarlyStopping(monitor='val_my_iou_metric', mode = 'max',patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric', mode = 'max',factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "history = model1.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[board, early_stopping, model_checkpoint,reduce_lr], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 800 samples\n",
      "Epoch 1/200\n",
      "6400/6400 [==============================] - 209s 33ms/step - loss: 0.3097 - my_iou_metric_2: 0.6662 - val_loss: 0.4208 - val_my_iou_metric_2: 0.6105\n",
      "\n",
      "Epoch 00001: val_my_iou_metric_2 improved from -inf to 0.61050, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 2/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: 0.1934 - my_iou_metric_2: 0.6839 - val_loss: 0.2837 - val_my_iou_metric_2: 0.6816\n",
      "\n",
      "Epoch 00002: val_my_iou_metric_2 improved from 0.61050 to 0.68162, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 3/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: 0.1461 - my_iou_metric_2: 0.6934 - val_loss: 0.3735 - val_my_iou_metric_2: 0.6634\n",
      "\n",
      "Epoch 00003: val_my_iou_metric_2 did not improve from 0.68162\n",
      "Epoch 4/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: 0.1158 - my_iou_metric_2: 0.6971 - val_loss: 0.3820 - val_my_iou_metric_2: 0.6558\n",
      "\n",
      "Epoch 00004: val_my_iou_metric_2 did not improve from 0.68162\n",
      "Epoch 5/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: 0.0628 - my_iou_metric_2: 0.7081 - val_loss: 0.1835 - val_my_iou_metric_2: 0.6661\n",
      "\n",
      "Epoch 00005: val_my_iou_metric_2 did not improve from 0.68162\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "Epoch 6/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.0555 - my_iou_metric_2: 0.7278 - val_loss: 1.3264 - val_my_iou_metric_2: 0.4693\n",
      "\n",
      "Epoch 00006: val_my_iou_metric_2 did not improve from 0.68162\n",
      "Epoch 7/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.0788 - my_iou_metric_2: 0.7334 - val_loss: 0.0247 - val_my_iou_metric_2: 0.7085\n",
      "\n",
      "Epoch 00007: val_my_iou_metric_2 improved from 0.68162 to 0.70850, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 8/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.0970 - my_iou_metric_2: 0.7383 - val_loss: 0.0605 - val_my_iou_metric_2: 0.7206\n",
      "\n",
      "Epoch 00008: val_my_iou_metric_2 improved from 0.70850 to 0.72062, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 9/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.1262 - my_iou_metric_2: 0.7427 - val_loss: -0.0298 - val_my_iou_metric_2: 0.7292\n",
      "\n",
      "Epoch 00009: val_my_iou_metric_2 improved from 0.72062 to 0.72925, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 10/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.1366 - my_iou_metric_2: 0.7432 - val_loss: -0.0644 - val_my_iou_metric_2: 0.7387\n",
      "\n",
      "Epoch 00010: val_my_iou_metric_2 improved from 0.72925 to 0.73875, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 11/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.1592 - my_iou_metric_2: 0.7445 - val_loss: -0.0494 - val_my_iou_metric_2: 0.7181\n",
      "\n",
      "Epoch 00011: val_my_iou_metric_2 did not improve from 0.73875\n",
      "Epoch 12/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.1747 - my_iou_metric_2: 0.7468 - val_loss: -0.1631 - val_my_iou_metric_2: 0.7466\n",
      "\n",
      "Epoch 00012: val_my_iou_metric_2 improved from 0.73875 to 0.74662, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 13/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.1868 - my_iou_metric_2: 0.7504 - val_loss: -0.1244 - val_my_iou_metric_2: 0.7375\n",
      "\n",
      "Epoch 00013: val_my_iou_metric_2 did not improve from 0.74662\n",
      "Epoch 14/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.2045 - my_iou_metric_2: 0.7532 - val_loss: -0.1425 - val_my_iou_metric_2: 0.7460\n",
      "\n",
      "Epoch 00014: val_my_iou_metric_2 did not improve from 0.74662\n",
      "Epoch 15/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.2106 - my_iou_metric_2: 0.7547 - val_loss: 0.1215 - val_my_iou_metric_2: 0.7036\n",
      "\n",
      "Epoch 00015: val_my_iou_metric_2 did not improve from 0.74662\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 16/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.2585 - my_iou_metric_2: 0.7625 - val_loss: -0.1812 - val_my_iou_metric_2: 0.7567\n",
      "\n",
      "Epoch 00016: val_my_iou_metric_2 improved from 0.74662 to 0.75675, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 17/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.2962 - my_iou_metric_2: 0.7664 - val_loss: -0.1717 - val_my_iou_metric_2: 0.7483\n",
      "\n",
      "Epoch 00017: val_my_iou_metric_2 did not improve from 0.75675\n",
      "Epoch 18/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.2918 - my_iou_metric_2: 0.7653 - val_loss: -0.1614 - val_my_iou_metric_2: 0.7556\n",
      "\n",
      "Epoch 00018: val_my_iou_metric_2 did not improve from 0.75675\n",
      "Epoch 19/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.3098 - my_iou_metric_2: 0.7679 - val_loss: -0.1594 - val_my_iou_metric_2: 0.7571\n",
      "\n",
      "Epoch 00019: val_my_iou_metric_2 improved from 0.75675 to 0.75712, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 20/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.3190 - my_iou_metric_2: 0.7722 - val_loss: -0.1423 - val_my_iou_metric_2: 0.7468\n",
      "\n",
      "Epoch 00020: val_my_iou_metric_2 did not improve from 0.75712\n",
      "Epoch 21/200\n",
      "6400/6400 [==============================] - 196s 31ms/step - loss: -0.3187 - my_iou_metric_2: 0.7699 - val_loss: -0.1420 - val_my_iou_metric_2: 0.7551\n",
      "\n",
      "Epoch 00021: val_my_iou_metric_2 did not improve from 0.75712\n",
      "Epoch 22/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.3320 - my_iou_metric_2: 0.7722 - val_loss: -0.1686 - val_my_iou_metric_2: 0.7535\n",
      "\n",
      "Epoch 00022: val_my_iou_metric_2 did not improve from 0.75712\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 23/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.3783 - my_iou_metric_2: 0.7795 - val_loss: -0.1983 - val_my_iou_metric_2: 0.7670\n",
      "\n",
      "Epoch 00023: val_my_iou_metric_2 improved from 0.75712 to 0.76700, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 24/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.3887 - my_iou_metric_2: 0.7822 - val_loss: -0.1772 - val_my_iou_metric_2: 0.7563\n",
      "\n",
      "Epoch 00024: val_my_iou_metric_2 did not improve from 0.76700\n",
      "Epoch 25/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.3930 - my_iou_metric_2: 0.7832 - val_loss: -0.1788 - val_my_iou_metric_2: 0.7551\n",
      "\n",
      "Epoch 00025: val_my_iou_metric_2 did not improve from 0.76700\n",
      "Epoch 26/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.3964 - my_iou_metric_2: 0.7838 - val_loss: -0.1951 - val_my_iou_metric_2: 0.7589\n",
      "\n",
      "Epoch 00026: val_my_iou_metric_2 did not improve from 0.76700\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 27/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4141 - my_iou_metric_2: 0.7867 - val_loss: -0.2162 - val_my_iou_metric_2: 0.7627\n",
      "\n",
      "Epoch 00027: val_my_iou_metric_2 did not improve from 0.76700\n",
      "Epoch 28/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4181 - my_iou_metric_2: 0.7892 - val_loss: -0.1907 - val_my_iou_metric_2: 0.7608\n",
      "\n",
      "Epoch 00028: val_my_iou_metric_2 did not improve from 0.76700\n",
      "Epoch 29/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4306 - my_iou_metric_2: 0.7870 - val_loss: -0.2045 - val_my_iou_metric_2: 0.7638\n",
      "\n",
      "Epoch 00029: val_my_iou_metric_2 did not improve from 0.76700\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Epoch 30/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4464 - my_iou_metric_2: 0.7896 - val_loss: -0.2127 - val_my_iou_metric_2: 0.7637\n",
      "\n",
      "Epoch 00030: val_my_iou_metric_2 did not improve from 0.76700\n",
      "Epoch 31/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4502 - my_iou_metric_2: 0.7918 - val_loss: -0.2066 - val_my_iou_metric_2: 0.7655\n",
      "\n",
      "Epoch 00031: val_my_iou_metric_2 did not improve from 0.76700\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4565 - my_iou_metric_2: 0.7912 - val_loss: -0.1978 - val_my_iou_metric_2: 0.7671\n",
      "\n",
      "Epoch 00032: val_my_iou_metric_2 improved from 0.76700 to 0.76712, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 33/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4534 - my_iou_metric_2: 0.7917 - val_loss: -0.2026 - val_my_iou_metric_2: 0.7677\n",
      "\n",
      "Epoch 00033: val_my_iou_metric_2 improved from 0.76712 to 0.76775, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 34/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4652 - my_iou_metric_2: 0.7956 - val_loss: -0.1896 - val_my_iou_metric_2: 0.7661\n",
      "\n",
      "Epoch 00034: val_my_iou_metric_2 did not improve from 0.76775\n",
      "Epoch 35/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4576 - my_iou_metric_2: 0.7917 - val_loss: -0.2105 - val_my_iou_metric_2: 0.7681\n",
      "\n",
      "Epoch 00035: val_my_iou_metric_2 improved from 0.76775 to 0.76812, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 36/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4636 - my_iou_metric_2: 0.7905 - val_loss: -0.1898 - val_my_iou_metric_2: 0.7639\n",
      "\n",
      "Epoch 00036: val_my_iou_metric_2 did not improve from 0.76812\n",
      "Epoch 37/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4724 - my_iou_metric_2: 0.7948 - val_loss: -0.1846 - val_my_iou_metric_2: 0.7639\n",
      "\n",
      "Epoch 00037: val_my_iou_metric_2 did not improve from 0.76812\n",
      "Epoch 38/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4634 - my_iou_metric_2: 0.7929 - val_loss: -0.2049 - val_my_iou_metric_2: 0.7644\n",
      "\n",
      "Epoch 00038: val_my_iou_metric_2 did not improve from 0.76812\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 39/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4654 - my_iou_metric_2: 0.7930 - val_loss: -0.2034 - val_my_iou_metric_2: 0.7681\n",
      "\n",
      "Epoch 00039: val_my_iou_metric_2 did not improve from 0.76812\n",
      "Epoch 40/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4711 - my_iou_metric_2: 0.7941 - val_loss: -0.1906 - val_my_iou_metric_2: 0.7671\n",
      "\n",
      "Epoch 00040: val_my_iou_metric_2 did not improve from 0.76812\n",
      "Epoch 41/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4683 - my_iou_metric_2: 0.7939 - val_loss: -0.1913 - val_my_iou_metric_2: 0.7689\n",
      "\n",
      "Epoch 00041: val_my_iou_metric_2 improved from 0.76812 to 0.76887, saving model to ../model/resunet_scale128_32_0.5.model\n",
      "Epoch 42/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4757 - my_iou_metric_2: 0.7930 - val_loss: -0.1895 - val_my_iou_metric_2: 0.7670\n",
      "\n",
      "Epoch 00042: val_my_iou_metric_2 did not improve from 0.76887\n",
      "Epoch 43/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4739 - my_iou_metric_2: 0.7945 - val_loss: -0.1866 - val_my_iou_metric_2: 0.7641\n",
      "\n",
      "Epoch 00043: val_my_iou_metric_2 did not improve from 0.76887\n",
      "Epoch 44/200\n",
      "6400/6400 [==============================] - 198s 31ms/step - loss: -0.4751 - my_iou_metric_2: 0.7927 - val_loss: -0.1924 - val_my_iou_metric_2: 0.7672\n",
      "\n",
      "Epoch 00044: val_my_iou_metric_2 did not improve from 0.76887\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "Epoch 45/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4758 - my_iou_metric_2: 0.7949 - val_loss: -0.1931 - val_my_iou_metric_2: 0.7637\n",
      "\n",
      "Epoch 00045: val_my_iou_metric_2 did not improve from 0.76887\n",
      "Epoch 46/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4803 - my_iou_metric_2: 0.7964 - val_loss: -0.1909 - val_my_iou_metric_2: 0.7650\n",
      "\n",
      "Epoch 00046: val_my_iou_metric_2 did not improve from 0.76887\n",
      "Epoch 47/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4827 - my_iou_metric_2: 0.7951 - val_loss: -0.1897 - val_my_iou_metric_2: 0.7660\n",
      "\n",
      "Epoch 00047: val_my_iou_metric_2 did not improve from 0.76887\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 48/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4801 - my_iou_metric_2: 0.7950 - val_loss: -0.1868 - val_my_iou_metric_2: 0.7668\n",
      "\n",
      "Epoch 00048: val_my_iou_metric_2 did not improve from 0.76887\n",
      "Epoch 49/200\n",
      "6400/6400 [==============================] - 198s 31ms/step - loss: -0.4821 - my_iou_metric_2: 0.7943 - val_loss: -0.1864 - val_my_iou_metric_2: 0.7661\n",
      "\n",
      "Epoch 00049: val_my_iou_metric_2 did not improve from 0.76887\n",
      "Epoch 50/200\n",
      "6400/6400 [==============================] - 198s 31ms/step - loss: -0.4777 - my_iou_metric_2: 0.7951 - val_loss: -0.1848 - val_my_iou_metric_2: 0.7657\n",
      "\n",
      "Epoch 00050: val_my_iou_metric_2 did not improve from 0.76887\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "Epoch 51/200\n",
      "6400/6400 [==============================] - 197s 31ms/step - loss: -0.4938 - my_iou_metric_2: 0.7978 - val_loss: -0.1890 - val_my_iou_metric_2: 0.7666\n",
      "\n",
      "Epoch 00051: val_my_iou_metric_2 did not improve from 0.76887\n",
      "Epoch 00051: early stopping\n"
     ]
    }
   ],
   "source": [
    "model1 = load_model(save_model_name,custom_objects={'my_iou_metric': my_iou_metric})\n",
    "# remove layter activation layer and use losvasz loss\n",
    "input_x = model1.layers[0].input\n",
    "output_layer = model1.layers[-1].input\n",
    "model = Model(input_x, output_layer)\n",
    "c = optimizers.adam(lr = 0.01)\n",
    "\n",
    "# lovasz_loss need input range (-∞，+∞), so cancel the last \"sigmoid\" activation  \n",
    "# Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\n",
    "model.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\n",
    "\n",
    "#model.summary()\n",
    "board = keras.callbacks.TensorBoard(log_dir='log/resunet_scale128_{}_{}'.format(start_feature, dropout),\n",
    "                       histogram_freq=0, write_graph=True, write_images=False)\n",
    "early_stopping = EarlyStopping(monitor='val_my_iou_metric_2', mode = 'max',patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric_2', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric_2', mode = 'max',factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n",
    "epochs = 200\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[board, model_checkpoint,reduce_lr,early_stopping], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n",
    "                                                   'lovasz_loss': lovasz_loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = predict_result(model,x_valid,img_size_target)\n",
    "## Scoring for last model, choose threshold by validation data \n",
    "thresholds_ori = np.linspace(0.3, 0.7, 31)\n",
    "# Reverse sigmoid function: Use code below because the  sigmoid activation was removed\n",
    "thresholds = np.log(thresholds_ori/(1-thresholds_ori)) \n",
    "\n",
    "# ious = np.array([get_iou_vector(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "# print(ious)\n",
    "ious = np.array([iou_metric_batch(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "print(ious)\n",
    "\n",
    "# instead of using default 0 as threshold, use validation data to find the best threshold.\n",
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale = True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\n",
    "\n",
    "preds_test = predict_result(model,x_test,img_size_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "pred_dict = {idx: rle_encode(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"Usedtime = {t2-t1} s\")\n",
    "\n",
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub = sub.reset_index()\n",
    "save_result(sub, '../result/resunet_scale128.csv', \n",
    "                        competition = 'tgs-salt-identification-challenge', \n",
    "                        send = False, index = False)\n",
    "\n",
    "t_finish = time.time()\n",
    "print(f\"Kernel run time = {(t_finish-t_start)/3600} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
