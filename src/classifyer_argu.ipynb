{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook #, tnrange\n",
    "#from itertools import chain\n",
    "from skimage.io import imread, imshow #, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add, Dense, Input, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\n",
    "\n",
    "import time\n",
    "from kaggle_util import *\n",
    "from models import *\n",
    "\n",
    "from albumentations import (\n",
    "    PadIfNeeded,\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,    \n",
    "    CenterCrop,    \n",
    "    Crop,\n",
    "    Compose,\n",
    "    Transpose,\n",
    "    RandomRotate90,\n",
    "    ElasticTransform,\n",
    "    GridDistortion, \n",
    "    OpticalDistortion,\n",
    "    RandomSizedCrop,\n",
    "    OneOf,\n",
    "    CLAHE,\n",
    "    RandomContrast,\n",
    "    RandomGamma,\n",
    "    RandomBrightness\n",
    ")\n",
    "\n",
    "\n",
    "t_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 101\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977aa801db624f95befafffaf9c966ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4850ce838aee4c7a97aec561930f1b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kownse/.local/lib/python3.6/site-packages/keras_preprocessing/image.py:487: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading of training/testing ids and depths\n",
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "\n",
    "len(train_df)\n",
    "\n",
    "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=False)) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
    "train_df['hassalt'] = train_df['masks'].apply(lambda x: (x.max()!=0) * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBSET = len(train_df)\n",
    "train_df = train_df.head(SUBSET)\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, msk_train, msk_val, y_train, y_valid, depth_train, depth_test = train_test_split(\n",
    "np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 3), \n",
    "np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "train_df.hassalt.values,\n",
    "train_df.z.values,\n",
    "test_size=0.2, stratify=train_df.coverage_class, random_state= 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9ead62af2145709a808678be2eb822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67828e013e534974912e663feffd0ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1663e42016d54eda8e13d3e922e37f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1b7035a0ed45bfa5c7f774fa971347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764444e6c735450b9df256be06ed6991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(9600, 101, 101, 3)\n",
      "(9600,)\n"
     ]
    }
   ],
   "source": [
    "aug_img = []\n",
    "aug_y = []\n",
    "augments = [\n",
    "    (0.5, HorizontalFlip(p=1)),\n",
    "#     (0.2, VerticalFlip(p=1)),\n",
    "#     (0.5, RandomRotate90(p=1)),\n",
    "#     (0.5, Transpose(p=1)),\n",
    "    (0.5, ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03)),\n",
    "    (0.5, GridDistortion(p=1)),\n",
    "    (0.5, OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)),\n",
    "#     (0.5, RandomSizedCrop(p=1, min_max_height=(int(img_size_ori / 2), img_size_ori), height=img_size_ori, width=img_size_ori)),\n",
    "]\n",
    "\n",
    "for ratio, aug in tqdm_notebook(augments):\n",
    "    selidx = np.random.choice(x_train.shape[0], int(x_train.shape[0] * ratio), replace=False)\n",
    "    for idx in tqdm_notebook(selidx):\n",
    "        augmented = aug(image=x_train[idx], mask=msk_train[idx])\n",
    "        aimg = augmented['image']\n",
    "        if len(aimg.shape) < 3:\n",
    "            aimg = aimg[...,np.newaxis]\n",
    "\n",
    "        aug_img.append(aimg)\n",
    "        aug_y.append(y_train[idx])\n",
    "\n",
    "aug_img = np.asarray(aug_img)\n",
    "aug_y = np.asarray(aug_y)\n",
    "x_train = np.append(x_train, aug_img, axis=0)\n",
    "y_train = np.append(y_train, aug_y, axis=0)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(BASE_MODEL):\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    if BASE_MODEL=='VGG16':\n",
    "        from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='RESNET52':\n",
    "        from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='InceptionV3':\n",
    "        from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='Xception':\n",
    "        from keras.applications.xception import Xception as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='DenseNet169': \n",
    "        from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='DenseNet121':\n",
    "        from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
    "    else:\n",
    "        raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
    "        \n",
    "    return PTModel, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(base_name = 'VGG16'):\n",
    "    print(base_name)\n",
    "    PTModel, preprocess_input = get_model(base_name)\n",
    "    x_train_act = x_train.copy()\n",
    "#     for idx in range(len(x_train_act)):\n",
    "#         x_train_act[idx] = preprocess_input(x_train_act[idx])\n",
    "\n",
    "    x_val_act = x_valid.copy()\n",
    "#     for idx in range(len(x_val_act)):\n",
    "#         x_val_act[idx] = preprocess_input(x_val_act[idx])\n",
    "\n",
    "    inputshape = (img_size_target, img_size_target, 3)\n",
    "    PTModel, preprocess_input = get_model('VGG16')\n",
    "    base_pretrained_model = PTModel(input_shape = inputshape, \n",
    "                                  include_top = False, weights = 'imagenet')\n",
    "    # base_pretrained_model.trainable = False\n",
    "\n",
    "    from keras import models, layers\n",
    "    from keras.optimizers import Adam\n",
    "    img_in = layers.Input(inputshape, name='Image_RGB_In')\n",
    "    x = base_pretrained_model(img_in)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchActivate(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchActivate(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    out_layer = layers.Dense(1, activation = 'sigmoid')(x)\n",
    "    class_model = models.Model(inputs = [img_in], outputs = [out_layer], name = 'full_model')\n",
    "\n",
    "    class_model.compile(optimizer = Adam(lr=0.01), \n",
    "                       loss = 'binary_crossentropy',\n",
    "                       metrics = ['binary_accuracy', my_iou_metric])\n",
    "\n",
    "    batch_size = 32\n",
    "    basic_name = '../model/classifier/{}_argu'.format(base_name)\n",
    "    save_model_name = basic_name + '.model'\n",
    "    submission_file = basic_name + '.csv'\n",
    "\n",
    "    print(save_model_name)\n",
    "    print(submission_file)\n",
    "\n",
    "\n",
    "    board = keras.callbacks.TensorBoard(log_dir='log/classifier/{}_argu'.format(base_name),\n",
    "                           histogram_freq=0, write_graph=True, write_images=False)\n",
    "    early_stopping = EarlyStopping(monitor='val_binary_accuracy', mode = 'max',patience=6, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_binary_accuracy', \n",
    "                                       mode = 'max', save_best_only=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_binary_accuracy', mode = 'max',factor=0.5, patience=2, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    epochs = 200\n",
    "\n",
    "    history = class_model.fit(x_train, y_train,\n",
    "                        validation_data=[x_valid, y_valid], \n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[board, early_stopping, model_checkpoint,reduce_lr], \n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xception\n",
      "../model/classifier/Xception_argu.model\n",
      "../model/classifier/Xception_argu.csv\n",
      "Train on 9600 samples, validate on 800 samples\n",
      "Epoch 1/200\n",
      "9600/9600 [==============================] - 33s 3ms/step - loss: 0.6755 - binary_accuracy: 0.5965 - my_iou_metric: 0.5965 - val_loss: 0.6687 - val_binary_accuracy: 0.6100 - val_my_iou_metric: 0.6100\n",
      "\n",
      "Epoch 00001: val_binary_accuracy improved from -inf to 0.61000, saving model to ../model/classifier/Xception_argu.model\n",
      "Epoch 2/200\n",
      "9600/9600 [==============================] - 30s 3ms/step - loss: 0.6124 - binary_accuracy: 0.6515 - my_iou_metric: 0.6515 - val_loss: 0.8523 - val_binary_accuracy: 0.6100 - val_my_iou_metric: 0.6100\n",
      "\n",
      "Epoch 00002: val_binary_accuracy did not improve from 0.61000\n",
      "Epoch 3/200\n",
      "9600/9600 [==============================] - 30s 3ms/step - loss: 0.5480 - binary_accuracy: 0.7210 - my_iou_metric: 0.7210 - val_loss: 0.7773 - val_binary_accuracy: 0.6100 - val_my_iou_metric: 0.6100\n",
      "\n",
      "Epoch 00003: val_binary_accuracy did not improve from 0.61000\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "Epoch 4/200\n",
      "8960/9600 [===========================>..] - ETA: 1s - loss: 0.5088 - binary_accuracy: 0.7587 - my_iou_metric: 0.7587"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-62fbe1abb590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Xception'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DenseNet169'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DenseNet121'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_lst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-beb338b411ab>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(base_name)\u001b[0m\n\u001b[1;32m     61\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                         verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_lst = ['Xception', 'DenseNet169', 'DenseNet121']\n",
    "for base_model in model_lst:\n",
    "    train_classifier(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n",
    "#                                                    'lovasz_loss': lovasz_loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_valid = predict_result(model,x_valid,img_size_target)\n",
    "# ## Scoring for last model, choose threshold by validation data \n",
    "# thresholds_ori = np.linspace(0.3, 0.7, 31)\n",
    "# # Reverse sigmoid function: Use code below because the  sigmoid activation was removed\n",
    "# thresholds = np.log(thresholds_ori/(1-thresholds_ori)) \n",
    "\n",
    "# # ious = np.array([get_iou_vector(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "# # print(ious)\n",
    "# ious = np.array([iou_metric_batch(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "# print(ious)\n",
    "\n",
    "# # instead of using default 0 as threshold, use validation data to find the best threshold.\n",
    "# threshold_best_index = np.argmax(ious) \n",
    "# iou_best = ious[threshold_best_index]\n",
    "# threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "# plt.plot(thresholds, ious)\n",
    "# plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "# plt.xlabel(\"Threshold\")\n",
    "# plt.ylabel(\"IoU\")\n",
    "# plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = np.array([(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale = True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\n",
    "\n",
    "# preds_test = predict_result(model,x_test,img_size_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
