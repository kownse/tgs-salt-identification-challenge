{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm #, tnrange\n",
    "#from itertools import chain\n",
    "from skimage.io import imread, imshow #, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\n",
    "\n",
    "from albumentations import (\n",
    "    PadIfNeeded,\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,    \n",
    "    CenterCrop,    \n",
    "    Crop,\n",
    "    Compose,\n",
    "    Transpose,\n",
    "    RandomRotate90,\n",
    "    ElasticTransform,\n",
    "    GridDistortion, \n",
    "    OpticalDistortion,\n",
    "    RandomSizedCrop,\n",
    "    OneOf,\n",
    "    CLAHE,\n",
    "    RandomContrast,\n",
    "    RandomGamma,\n",
    "    RandomBrightness\n",
    ")\n",
    "\n",
    "import time\n",
    "from kaggle_util import *\n",
    "from models import *\n",
    "\n",
    "t_start = time.time()\n",
    "start_feature = 32\n",
    "batch_size = 16\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 101\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "\n",
    "def build_model_deeper(input_layer, start_neurons, DropoutRatio = 0.5):\n",
    "    # 101 -> 50\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(input_layer)\n",
    "    conv1 = residual_block(conv1,start_neurons * 1)\n",
    "    conv1 = residual_block(conv1,start_neurons * 1)\n",
    "    conv1 = residual_block(conv1,start_neurons * 1, True, act = 'elu')\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(DropoutRatio/2)(pool1)\n",
    "\n",
    "    # 50 -> 25\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n",
    "    conv2 = residual_block(conv2,start_neurons * 2)\n",
    "    conv2 = residual_block(conv2,start_neurons * 2)\n",
    "    conv2 = residual_block(conv2,start_neurons * 2, True, act = 'elu')\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(DropoutRatio)(pool2)\n",
    "\n",
    "    # 25 -> 12\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n",
    "    conv3 = residual_block(conv3,start_neurons * 4)\n",
    "    conv3 = residual_block(conv3,start_neurons * 4)\n",
    "    conv3 = residual_block(conv3,start_neurons * 4, True, act = 'elu')\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(DropoutRatio)(pool3)\n",
    "\n",
    "    # 12 -> 6\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n",
    "    conv4 = residual_block(conv4,start_neurons * 8)\n",
    "    conv4 = residual_block(conv4,start_neurons * 8)\n",
    "    conv4 = residual_block(conv4,start_neurons * 8, True, act = 'elu')\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(DropoutRatio)(pool4)\n",
    "\n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n",
    "    convm = residual_block(convm,start_neurons * 16)\n",
    "    convm = residual_block(convm,start_neurons * 16)\n",
    "    convm = residual_block(convm,start_neurons * 16, True, act = 'elu')\n",
    "    \n",
    "    # 6 -> 12\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(DropoutRatio)(uconv4)\n",
    "    \n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 8)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 8)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 8, True, act = 'elu')\n",
    "    \n",
    "    # 12 -> 25\n",
    "    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])    \n",
    "    uconv3 = Dropout(DropoutRatio)(uconv3)\n",
    "    \n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 4)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 4)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 4, True, act = 'elu')\n",
    "\n",
    "    # 25 -> 50\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "        \n",
    "    uconv2 = Dropout(DropoutRatio)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 2, True, act = 'elu')\n",
    "    \n",
    "    # 50 -> 101\n",
    "    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    \n",
    "    uconv1 = Dropout(DropoutRatio)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 1, True, act = 'elu')\n",
    "    \n",
    "    #uconv1 = Dropout(DropoutRatio/2)(uconv1)\n",
    "    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\n",
    "    output_layer =  Activation('sigmoid')(output_layer_noActi)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of training/testing ids and depths\n",
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "\n",
    "unfit_df = pd.read_csv(\"../result/iou_elu.csv\")\n",
    "train_df = train_df.reset_index()\n",
    "train_unfit = pd.merge(unfit_df, train_df, on='id', how = 'left')\n",
    "train_df = train_unfit\n",
    "train_df = train_df.set_index('id')\n",
    "train_df['unfit'] = (train_df['iou'] < 0.7) * 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]/home/kownse/.local/lib/python3.6/site-packages/keras_preprocessing/image.py:487: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n",
      "100%|██████████| 4000/4000 [00:02<00:00, 1509.09it/s]\n",
      "100%|██████████| 4000/4000 [00:01<00:00, 2270.62it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm(train_df.index)]\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
    "train_df['hassalt'] = train_df['masks'].apply(lambda x: (x.max()!=0) * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(train_df, train_idx, val_idx):\n",
    "    X_train = train_df.iloc[train_idx]\n",
    "    X_valid = train_df.iloc[val_idx]\n",
    "    x_train = np.array(X_train.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "    x_valid = np.array(X_valid.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "    msk_train = np.array(X_train.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "    msk_val = np.array(X_valid.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n",
    "    y_train = X_train.hassalt.values\n",
    "    y_valid = X_valid.hassalt.values\n",
    "    id_train = X_train.index.values\n",
    "    id_valid = X_valid.index.values\n",
    "    return x_train, x_valid, msk_train, msk_val, y_train, y_valid, id_train, id_valid\n",
    "\n",
    "def argument(x_train, msk_train, y_train):\n",
    "    aug_img = []\n",
    "    aug_msk = []\n",
    "    aug_y = []\n",
    "    augments = [\n",
    "        (1, HorizontalFlip(p=1)),\n",
    "#         (0.25, VerticalFlip(p=1)),\n",
    "#         (0.25, RandomRotate90(p=1)),\n",
    "#         (0.25, Transpose(p=1)),\n",
    "#         (0.25, ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03)),\n",
    "#         (0.25, GridDistortion(p=1)),\n",
    "#         (0.25, OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)),\n",
    "#         (0.5, RandomSizedCrop(p=1, min_max_height=(int(img_size_ori / 2), img_size_ori), height=img_size_ori, width=img_size_ori)),\n",
    "    ]\n",
    "\n",
    "    for ratio, aug in tqdm(augments):\n",
    "        selidx = np.random.choice(x_train.shape[0], int(x_train.shape[0] * ratio), replace=False)\n",
    "        for idx in tqdm(selidx):\n",
    "            augmented = aug(image=x_train[idx], mask=msk_train[idx])\n",
    "            aimg = augmented['image']\n",
    "            amsk = augmented['mask']\n",
    "            if len(aimg.shape) < 3:\n",
    "                aimg = aimg[...,np.newaxis]\n",
    "            if len(amsk.shape) < 3:\n",
    "                amsk = amsk[...,np.newaxis]\n",
    "            aug_img.append(aimg)\n",
    "            aug_msk.append(amsk)\n",
    "            aug_y.append(y_train[idx])\n",
    "\n",
    "    aug_img = np.asarray(aug_img)\n",
    "    aug_msk = np.asarray(aug_msk)\n",
    "    aug_y = np.asarray(aug_y)\n",
    "    x_train = np.append(x_train, aug_img, axis=0)\n",
    "    msk_train = np.append(msk_train, aug_msk, axis=0)\n",
    "    y_train = np.append(y_train, aug_y, axis=0)\n",
    "    print(x_train.shape)\n",
    "    print(msk_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    return x_train, msk_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(fold, x_train, y_train, x_valid, y_valid, id_valid):\n",
    "    \n",
    "    cur_base_name = 'Unet_resnet_deeper_elu_fix2_{}_{}_{}_{}'.format(start_feature, batch_size, dropout, fold)\n",
    "    cur_save_model_name = '../model/segmenter/{}.model'.format(cur_base_name)\n",
    "    if False: #os.path.exists(cur_save_model_name):\n",
    "        print('load from continue', cur_save_model_name)\n",
    "        model = load_model(cur_save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n",
    "                                                   'lovasz_loss': lovasz_loss})\n",
    "    else:\n",
    "    \n",
    "        base_name = 'Unet_resnet_deeper_elu_fix_{}_{}_{}_{}'.format(start_feature, batch_size, dropout, 3)\n",
    "        save_model_name = '../model/segmenter/{}.model'.format(base_name)\n",
    "        submission_dir = '../result/segmenter/{}.csv'.format(base_name)\n",
    "        oof_dir = '../result/segmenter_oof/{}'.format(base_name)\n",
    "\n",
    "        print('load from last run', save_model_name)\n",
    "        model = load_model(save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n",
    "                                                       'lovasz_loss': lovasz_loss})\n",
    "    \n",
    "    base_name = 'Unet_resnet_deeper_elu_fix2_{}_{}_{}_{}'.format(start_feature, batch_size, dropout, fold)\n",
    "    save_model_name = '../model/segmenter/{}.model'.format(base_name)\n",
    "    submission_dir = '../result/segmenter/{}.csv'.format(base_name)\n",
    "    oof_dir = '../result/segmenter_oof/{}'.format(base_name)\n",
    "\n",
    "    print(save_model_name)\n",
    "    print(submission_dir)\n",
    "    print(oof_dir)\n",
    "\n",
    "    c = optimizers.adam(lr = 0.01)\n",
    "#     c = optimizers.SGD(lr = 0.01, momentum=0.9, decay=0.0001)\n",
    "    model.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\n",
    "\n",
    "    #model.summary()\n",
    "    board = keras.callbacks.TensorBoard(log_dir='log/segmenter/{}'.format(base_name),\n",
    "                           histogram_freq=0, write_graph=True, write_images=False)\n",
    "    early_stopping = EarlyStopping(monitor='val_my_iou_metric_2', mode = 'max',patience=20, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric_2', \n",
    "                                       mode = 'max', save_best_only=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric_2', mode = 'max',factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "    epochs = 200\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=[x_valid, y_valid], \n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[board, model_checkpoint,reduce_lr,early_stopping], \n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/3199 [00:00<?, ?it/s]\u001b[A\n",
      " 18%|█▊        | 571/3199 [00:00<00:00, 5658.27it/s]\u001b[A\n",
      " 35%|███▌      | 1125/3199 [00:00<00:00, 5597.71it/s]\u001b[A\n",
      " 52%|█████▏    | 1677/3199 [00:00<00:00, 5567.80it/s]\u001b[A\n",
      " 71%|███████   | 2264/3199 [00:00<00:00, 5642.65it/s]\u001b[A\n",
      " 88%|████████▊ | 2826/3199 [00:00<00:00, 5635.50it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.75it/s].39it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6398, 101, 101, 1)\n",
      "(6398, 101, 101, 1)\n",
      "(6398,)\n",
      "load from last run ../model/segmenter/Unet_resnet_deeper_elu_fix_32_16_0.5_3.model\n",
      "../model/segmenter/Unet_resnet_deeper_elu_fix2_32_16_0.5_0.model\n",
      "../result/segmenter/Unet_resnet_deeper_elu_fix2_32_16_0.5_0.csv\n",
      "../result/segmenter_oof/Unet_resnet_deeper_elu_fix2_32_16_0.5_0\n",
      "Train on 6398 samples, validate on 801 samples\n",
      "Epoch 1/200\n",
      "6398/6398 [==============================] - 118s 18ms/step - loss: -0.5315 - my_iou_metric_2: 0.8744 - val_loss: -0.4554 - val_my_iou_metric_2: 0.8549\n",
      "\n",
      "Epoch 00001: val_my_iou_metric_2 improved from -inf to 0.85493, saving model to ../model/segmenter/Unet_resnet_deeper_elu_fix2_32_16_0.5_0.model\n",
      "Epoch 2/200\n",
      "6398/6398 [==============================] - 105s 16ms/step - loss: -0.5340 - my_iou_metric_2: 0.8709 - val_loss: -0.5511 - val_my_iou_metric_2: 0.8723\n",
      "\n",
      "Epoch 00002: val_my_iou_metric_2 improved from 0.85493 to 0.87228, saving model to ../model/segmenter/Unet_resnet_deeper_elu_fix2_32_16_0.5_0.model\n",
      "Epoch 3/200\n",
      "6398/6398 [==============================] - 105s 16ms/step - loss: -0.5326 - my_iou_metric_2: 0.8720 - val_loss: -0.4015 - val_my_iou_metric_2: 0.8422\n",
      "\n",
      "Epoch 00003: val_my_iou_metric_2 did not improve from 0.87228\n",
      "Epoch 4/200\n",
      "6398/6398 [==============================] - 104s 16ms/step - loss: -0.5415 - my_iou_metric_2: 0.8748 - val_loss: -0.4884 - val_my_iou_metric_2: 0.8600\n",
      "\n",
      "Epoch 00004: val_my_iou_metric_2 did not improve from 0.87228\n",
      "Epoch 5/200\n",
      "6398/6398 [==============================] - 105s 16ms/step - loss: -0.5430 - my_iou_metric_2: 0.8727 - val_loss: -0.4873 - val_my_iou_metric_2: 0.8643\n",
      "\n",
      "Epoch 00005: val_my_iou_metric_2 did not improve from 0.87228\n",
      "Epoch 6/200\n",
      "6398/6398 [==============================] - 105s 16ms/step - loss: -0.5765 - my_iou_metric_2: 0.8807 - val_loss: -0.2800 - val_my_iou_metric_2: 0.8345\n",
      "\n",
      "Epoch 00006: val_my_iou_metric_2 did not improve from 0.87228\n",
      "Epoch 7/200\n",
      "6398/6398 [==============================] - 105s 16ms/step - loss: -0.5695 - my_iou_metric_2: 0.8796 - val_loss: -0.4512 - val_my_iou_metric_2: 0.8610\n",
      "\n",
      "Epoch 00007: val_my_iou_metric_2 did not improve from 0.87228\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "Epoch 8/200\n",
      "6398/6398 [==============================] - 105s 16ms/step - loss: -0.6341 - my_iou_metric_2: 0.8950 - val_loss: -0.6030 - val_my_iou_metric_2: 0.8828\n",
      "\n",
      "Epoch 00008: val_my_iou_metric_2 improved from 0.87228 to 0.88277, saving model to ../model/segmenter/Unet_resnet_deeper_elu_fix2_32_16_0.5_0.model\n",
      "Epoch 9/200\n",
      "6398/6398 [==============================] - 105s 16ms/step - loss: -0.6663 - my_iou_metric_2: 0.9027 - val_loss: -0.5604 - val_my_iou_metric_2: 0.8762\n",
      "\n",
      "Epoch 00009: val_my_iou_metric_2 did not improve from 0.88277\n",
      "Epoch 10/200\n",
      "6398/6398 [==============================] - 106s 17ms/step - loss: -0.6782 - my_iou_metric_2: 0.9050 - val_loss: -0.6021 - val_my_iou_metric_2: 0.8833\n",
      "\n",
      "Epoch 00010: val_my_iou_metric_2 improved from 0.88277 to 0.88327, saving model to ../model/segmenter/Unet_resnet_deeper_elu_fix2_32_16_0.5_0.model\n",
      "Epoch 11/200\n",
      "5648/6398 [=========================>....] - ETA: 11s - loss: -0.6729 - my_iou_metric_2: 0.9055"
     ]
    }
   ],
   "source": [
    "folds = [0,1,2,3,4]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['hassalt'])):\n",
    "    if fold not in folds:\n",
    "        print('skip fold', fold)\n",
    "        continue\n",
    "        \n",
    "    x_train, x_valid, msk_train, msk_val, y_train, y_valid, id_train, id_valid = get_splits(train_df, train_idx, val_idx)\n",
    "    x_train, msk_train, y_train = argument(x_train, msk_train, y_train)\n",
    "    \n",
    "    model = train_model(fold, x_train, msk_train, x_valid, msk_val, id_valid)\n",
    "    \n",
    "    from keras import backend as K\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals = []\n",
    "msk_vals = []\n",
    "pred_tests = []\n",
    "x_test = np.array([(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale = True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['hassalt'])):\n",
    "    print('predict for fold', fold)\n",
    "    \n",
    "    \n",
    "    base_name = 'Unet_resnet_deeper_elu_fix2_{}_{}_{}_{}'.format(start_feature, batch_size, dropout, fold)\n",
    "    basic_name = '../model/segmenter/{}'.format(base_name)\n",
    "    save_model_name = basic_name + '.model'\n",
    "\n",
    "    model = load_model(save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n",
    "                                                   'lovasz_loss': lovasz_loss})\n",
    "    \n",
    "    x_train, x_valid, msk_train, msk_val, y_train, y_valid, id_train, id_valid = get_splits(train_df, train_idx, \n",
    "                                                                                            val_idx)\n",
    "    \n",
    "    preds_valid = predict_result(model,x_valid,img_size_target)\n",
    "    pred_vals.append(preds_valid)\n",
    "    msk_vals.append(msk_val)\n",
    "    \n",
    "    preds_test = predict_result(model,x_test,img_size_target)\n",
    "    pred_tests.append(preds_test)\n",
    "\n",
    "    from keras import backend as K\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['hassalt'])):\n",
    "    val_ids += list(train_df.iloc[val_idx].index.values)\n",
    "    \n",
    "pred_all = np.concatenate(pred_vals, axis=0)\n",
    "msk_all = np.concatenate(msk_vals, axis= 0)\n",
    "\n",
    "thresholds_ori = np.linspace(0.3, 0.7, 35)\n",
    "thresholds = np.log(thresholds_ori/(1-thresholds_ori)) \n",
    "\n",
    "# ious = np.array([get_iou_vector(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "# print(ious)\n",
    "ious = np.array([iou_metric_batch(msk_all, pred_all > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "print(ious)\n",
    "\n",
    "# instead of using default 0 as threshold, use validation data to find the best threshold.\n",
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_all = (pred_all > threshold_best) * 1\n",
    "df_iou = pd.DataFrame(columns=['id', 'iou'])\n",
    "cnt = 0\n",
    "for i in tqdm_notebook(range(len(val_all))):\n",
    "    id = val_ids[i]\n",
    "    iou = iou_metric_batch(msk_all[i], val_all[i])\n",
    "    df_iou = df_iou.append({'id':id, 'iou':iou}, ignore_index=True)\n",
    "\n",
    "df_iou.to_csv('../result/iou_elu.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tests_act = pred_tests\n",
    "pred_test_all = np.zeros_like(pred_tests_act[0], dtype=np.float32)\n",
    "for pre in pred_tests_act:\n",
    "    pred_test_all += pre / len(pred_tests_act)\n",
    "    \n",
    "pred_dict = {idx: rle_encode(np.round(downsample(pred_test_all[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\n",
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub = sub.reset_index()\n",
    "save_result(sub, '../result/segmenter/unet_resnet_elu_fix2_avg.csv', \n",
    "                        competition = 'tgs-salt-identification-challenge', \n",
    "                        send = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
