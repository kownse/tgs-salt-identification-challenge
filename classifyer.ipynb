{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook #, tnrange\n",
    "#from itertools import chain\n",
    "from skimage.io import imread, imshow #, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add, Dense, Input, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\n",
    "\n",
    "import time\n",
    "from kaggle_util import *\n",
    "from models import *\n",
    "\n",
    "from albumentations import (\n",
    "    PadIfNeeded,\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,    \n",
    "    CenterCrop,    \n",
    "    Crop,\n",
    "    Compose,\n",
    "    Transpose,\n",
    "    RandomRotate90,\n",
    "    ElasticTransform,\n",
    "    GridDistortion, \n",
    "    OpticalDistortion,\n",
    "    RandomSizedCrop,\n",
    "    OneOf,\n",
    "    CLAHE,\n",
    "    RandomContrast,\n",
    "    RandomGamma,\n",
    "    RandomBrightness\n",
    ")\n",
    "\n",
    "t_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 101\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbfdde3dce94fe2b5d20d8c88e04938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c868b87b323641fc91df7987be53ed9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kownse/.local/lib/python3.6/site-packages/keras_preprocessing/image.py:487: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading of training/testing ids and depths\n",
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "\n",
    "len(train_df)\n",
    "\n",
    "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=False)) for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
    "train_df['hassalt'] = train_df['masks'].apply(lambda x: (x.max()!=0) * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_max = train_df['z'].max()\n",
    "z_min = train_df['z'].min()\n",
    "z_dis = z_max - z_min\n",
    "train_df['z'] = (train_df['z'] - z_min) / z_dis\n",
    "step = 1 / z_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBSET = len(train_df)\n",
    "train_df = train_df.head(SUBSET)\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, msk_train, msk_val, y_train, y_valid, depth_train, depth_test = train_test_split(\n",
    "np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 3), \n",
    "np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "train_df.hassalt.values,\n",
    "train_df.z.values,\n",
    "test_size=0.2, stratify=train_df.coverage_class, random_state= 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_img = []\n",
    "# aug_y = []\n",
    "# augments = [\n",
    "#     (0.5, HorizontalFlip(p=1)),\n",
    "#     (0.5, VerticalFlip(p=1)),\n",
    "#     (0.5, RandomRotate90(p=1)),\n",
    "#     (0.5, Transpose(p=1)),\n",
    "#     (0.5, ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03)),\n",
    "#     (0.5, GridDistortion(p=1)),\n",
    "#     (0.5, OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)),\n",
    "#     (0.5, RandomSizedCrop(p=1, min_max_height=(int(img_size_ori / 2), img_size_ori), height=img_size_ori, width=img_size_ori)),\n",
    "# ]\n",
    "\n",
    "# for ratio, aug in tqdm_notebook(augments):\n",
    "#     selidx = np.random.choice(x_train.shape[0], int(x_train.shape[0] * ratio), replace=False)\n",
    "#     for idx in tqdm_notebook(selidx):\n",
    "#         augmented = aug(image=x_train[idx], mask=msk_train[idx])\n",
    "#         aimg = augmented['image']\n",
    "#         if len(aimg.shape) < 3:\n",
    "#             aimg = aimg[...,np.newaxis]\n",
    "\n",
    "#         aug_img.append(aimg)\n",
    "#         aug_y.append(y_train[idx])\n",
    "\n",
    "# aug_img = np.asarray(aug_img)\n",
    "# aug_y = np.asarray(aug_y)\n",
    "# x_train = np.append(x_train, aug_img, axis=0)\n",
    "# y_train = np.append(y_train, aug_y, axis=0)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "# y_train = np.append(y_train, y_train, axis=0)\n",
    "# depth_train = np.append(depth_train, depth_train, axis=0)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = add_depth_bulk_act(x_train, depth_train, step)\n",
    "# x_valid = add_depth_bulk_act(x_valid, depth_test, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(BASE_MODEL):\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    if BASE_MODEL=='VGG16':\n",
    "        from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='VGG19':\n",
    "        from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='RESNET52':\n",
    "        from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='InceptionV3':\n",
    "        from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='Xception':\n",
    "        from keras.applications.xception import Xception as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='DenseNet169': \n",
    "        from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='DenseNet121':\n",
    "        from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
    "    elif BASE_MODEL=='InceptionResNetV2':\n",
    "        from keras.applications.inception_resnet_v2 import InceptionResNetV2 as PTModel, preprocess_input\n",
    "    else:\n",
    "        raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
    "        \n",
    "    return PTModel, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kownse/.local/lib/python3.6/site-packages/keras_applications/resnet50.py:264: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(input_shape = (101,101,3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x7fc7b7f3b0f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(base_name = 'RESNET52'):\n",
    "    print(base_name)\n",
    "    PTModel, preprocess_input = get_model(base_name)\n",
    "    x_train_act = x_train\n",
    "    x_val_act = x_valid\n",
    "#     x_train_act = x_train.astype(np.float32)\n",
    "#     for idx in range(len(x_train_act)):\n",
    "#         x_train_act[idx] = preprocess_input(x_train_act[idx])\n",
    "#     print(x_train_act.max())\n",
    "\n",
    "#     x_val_act = x_valid.astype(np.float32)\n",
    "#     for idx in range(len(x_val_act)):\n",
    "#         x_val_act[idx] = preprocess_input(x_val_act[idx])\n",
    "\n",
    "    inputshape = x_train_act.shape[1:]\n",
    "    PTModel, preprocess_input = get_model(base_name)\n",
    "    base_pretrained_model = PTModel(input_shape = inputshape, \n",
    "                                  include_top = False, weights = 'imagenet')\n",
    "    base_pretrained_model.trainable = False\n",
    "\n",
    "    from keras import models, layers\n",
    "    from keras.optimizers import Adam\n",
    "    img_in = layers.Input(inputshape, name='Image_RGB_In')\n",
    "    x = base_pretrained_model(img_in)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.GlobalMaxPooling2D()(x)\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchActivate(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchActivate(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    out_layer = layers.Dense(1, activation = 'sigmoid')(x)\n",
    "    class_model = models.Model(inputs = [img_in], outputs = [out_layer], name = 'full_model')\n",
    "\n",
    "    class_model.compile(optimizer = Adam(lr=0.01), \n",
    "                       loss = 'binary_crossentropy',\n",
    "                       metrics = ['binary_accuracy'])\n",
    "\n",
    "    batch_size = 32\n",
    "    basic_name = '../model/classifier/{}'.format(base_name)\n",
    "    save_model_name = basic_name + '.model'\n",
    "    submission_file = basic_name + '.csv'\n",
    "\n",
    "    print(save_model_name)\n",
    "    print(submission_file)\n",
    "\n",
    "\n",
    "    board = keras.callbacks.TensorBoard(log_dir='log/classifier/{}'.format(base_name),\n",
    "                           histogram_freq=0, write_graph=True, write_images=False)\n",
    "    early_stopping = EarlyStopping(monitor='val_binary_accuracy', mode = 'max',patience=5, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_binary_accuracy', \n",
    "                                       mode = 'max', save_best_only=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_binary_accuracy', mode = 'max',factor=0.5, patience=2, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    epochs = 200\n",
    "\n",
    "    history = class_model.fit(x_train, y_train,\n",
    "                        validation_data=[x_valid, y_valid], \n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[board, early_stopping, reduce_lr, model_checkpoint],\n",
    "                        verbose=1)\n",
    "    \n",
    "    model = load_model('../model/classifier/Xception.model',custom_objects={'my_iou_metric': my_iou_metric})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16\n",
      "151.061\n",
      "../model/classifier/VGG16.model\n",
      "../model/classifier/VGG16.csv\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/200\n",
      "3200/3200 [==============================] - 7s 2ms/step - loss: 0.4163 - binary_accuracy: 0.8147 - val_loss: 0.3189 - val_binary_accuracy: 0.8662\n",
      "\n",
      "Epoch 00001: val_binary_accuracy improved from -inf to 0.86625, saving model to ../model/classifier/VGG16.model\n",
      "Epoch 2/200\n",
      "3200/3200 [==============================] - 5s 2ms/step - loss: 0.3014 - binary_accuracy: 0.8728 - val_loss: 0.3233 - val_binary_accuracy: 0.8538\n",
      "\n",
      "Epoch 00002: val_binary_accuracy did not improve from 0.86625\n",
      "Epoch 3/200\n",
      "3200/3200 [==============================] - 5s 2ms/step - loss: 0.2629 - binary_accuracy: 0.8875 - val_loss: 0.3117 - val_binary_accuracy: 0.8650\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00003: val_binary_accuracy did not improve from 0.86625\n",
      "Epoch 4/200\n",
      "3200/3200 [==============================] - 5s 2ms/step - loss: 0.1887 - binary_accuracy: 0.9281 - val_loss: 0.3202 - val_binary_accuracy: 0.8775\n",
      "\n",
      "Epoch 00004: val_binary_accuracy improved from 0.86625 to 0.87750, saving model to ../model/classifier/VGG16.model\n",
      "Epoch 5/200\n",
      "3200/3200 [==============================] - 5s 2ms/step - loss: 0.1675 - binary_accuracy: 0.9322 - val_loss: 0.3199 - val_binary_accuracy: 0.8788\n",
      "\n",
      "Epoch 00005: val_binary_accuracy improved from 0.87750 to 0.87875, saving model to ../model/classifier/VGG16.model\n",
      "Epoch 6/200\n",
      "3200/3200 [==============================] - 5s 2ms/step - loss: 0.1419 - binary_accuracy: 0.9484 - val_loss: 0.3670 - val_binary_accuracy: 0.8625\n",
      "\n",
      "Epoch 00006: val_binary_accuracy did not improve from 0.87875\n",
      "Epoch 7/200\n",
      "3200/3200 [==============================] - 5s 2ms/step - loss: 0.1271 - binary_accuracy: 0.9503 - val_loss: 0.4342 - val_binary_accuracy: 0.8612\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00007: val_binary_accuracy did not improve from 0.87875\n",
      "Epoch 8/200\n",
      "3200/3200 [==============================] - 5s 2ms/step - loss: 0.1095 - binary_accuracy: 0.9603 - val_loss: 0.4167 - val_binary_accuracy: 0.8762\n",
      "\n",
      "Epoch 00008: val_binary_accuracy did not improve from 0.87875\n",
      "Epoch 9/200\n",
      "3200/3200 [==============================] - 5s 2ms/step - loss: 0.0907 - binary_accuracy: 0.9603 - val_loss: 0.4293 - val_binary_accuracy: 0.8688\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00009: val_binary_accuracy did not improve from 0.87875\n",
      "Epoch 10/200\n",
      "3200/3200 [==============================] - 5s 2ms/step - loss: 0.0655 - binary_accuracy: 0.9762 - val_loss: 0.4427 - val_binary_accuracy: 0.8700\n",
      "\n",
      "Epoch 00010: val_binary_accuracy did not improve from 0.87875\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_lst = ['VGG16']\n",
    "for base_model in model_lst:\n",
    "    train_classifier(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kownse/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:304: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../model/classifier/Xception.model',custom_objects={'my_iou_metric': my_iou_metric})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n",
    "#                                                    'lovasz_loss': lovasz_loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aafa851011d4c4fbc6ddf64e65ef661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.61    0.61    0.61    0.61    0.61    0.61    0.61    0.61    0.61\n",
      " 0.61    0.61    0.61    0.61    0.61    0.61    0.61    0.4425  0.435\n",
      " 0.4325  0.42625 0.4225  0.42    0.415   0.41125 0.42375 0.4175  0.4125\n",
      " 0.41125 0.40875 0.405   0.40375]\n",
      "Threshold vs IoU (-0.8472978603872036, 0.61)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc4f03ae198>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_valid = model.predict(x_valid)\n",
    "## Scoring for last model, choose threshold by validation data \n",
    "thresholds_ori = np.linspace(0.3, 0.7, 31)\n",
    "# Reverse sigmoid function: Use code below because the  sigmoid activation was removed\n",
    "thresholds = np.log(thresholds_ori/(1-thresholds_ori)) \n",
    "\n",
    "# ious = np.array([get_iou_vector(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "# print(ious)\n",
    "ious = np.array([iou_metric_batch(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "print(ious)\n",
    "\n",
    "# instead of using default 0 as threshold, use validation data to find the best threshold.\n",
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "title = \"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best)\n",
    "print(title)\n",
    "plt.title(title)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = np.array([(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale = True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\n",
    "\n",
    "# preds_test = predict_result(model,x_test,img_size_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
